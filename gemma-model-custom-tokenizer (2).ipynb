{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11122387,"sourceType":"datasetVersion","datasetId":6935879}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:41:46.262176Z","iopub.execute_input":"2025-04-29T07:41:46.262810Z","iopub.status.idle":"2025-04-29T07:41:46.589361Z","shell.execute_reply.started":"2025-04-29T07:41:46.262784Z","shell.execute_reply":"2025-04-29T07:41:46.588413Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/abs-dataset/dataset_final.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/abs-dataset/dataset_final.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:41:46.590545Z","iopub.execute_input":"2025-04-29T07:41:46.591084Z","iopub.status.idle":"2025-04-29T07:41:46.772932Z","shell.execute_reply.started":"2025-04-29T07:41:46.591050Z","shell.execute_reply":"2025-04-29T07:41:46.772112Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"texts=train['Text'].tolist()\nimport pandas as pd\nfrom tokenizers import ByteLevelBPETokenizer\nimport os\n\n\n# Extract text column (Replace 'text_column' with the actual column name)\ntext_data = train[\"Text\"].dropna().tolist()\n\n# Save as a single text file for training the tokenizer\nwith open(\"dataset.txt\", \"w\", encoding=\"utf-8\") as f:\n    for line in text_data:\n        f.write(line + \"\\n\")  # Write each text entry as a new line\n\n# Initialize tokenizer\ntokenizer = ByteLevelBPETokenizer()\n\n# Train the tokenizer on the extracted text\ntokenizer.train(files=[\"dataset.txt\"], vocab_size=52000, min_frequency=2)\nos.makedirs('.tokenizer_gpt',exist_ok=True)\n\n# Save the tokenizer model\ntokenizer.save_model(\".tokenizer_gpt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:41:46.777327Z","iopub.execute_input":"2025-04-29T07:41:46.777497Z","iopub.status.idle":"2025-04-29T07:41:49.057256Z","shell.execute_reply.started":"2025-04-29T07:41:46.777483Z","shell.execute_reply":"2025-04-29T07:41:49.056655Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['.tokenizer_gpt/vocab.json', '.tokenizer_gpt/merges.txt']"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Save the tokenizer vocabulary (vocab.json) and merges.txt\ntokenizer.save(\".tokenizer_gpt/vocab.json\", pretty=True) # Save only vocab.json\ntokenizer.save(\".tokenizer_gpt/merges.txt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:41:49.057997Z","iopub.execute_input":"2025-04-29T07:41:49.058248Z","iopub.status.idle":"2025-04-29T07:41:49.071091Z","shell.execute_reply.started":"2025-04-29T07:41:49.058221Z","shell.execute_reply":"2025-04-29T07:41:49.070352Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast\ntokenizer_file_path = os.path.abspath(\".tokenizer_gpt/vocab.json\")\n\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_file=tokenizer_file_path \n)\n\n# Add padding token if not present\ntokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:41:49.072197Z","iopub.execute_input":"2025-04-29T07:41:49.072679Z","iopub.status.idle":"2025-04-29T07:41:51.217306Z","shell.execute_reply.started":"2025-04-29T07:41:49.072661Z","shell.execute_reply":"2025-04-29T07:41:51.216537Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import transformers\ntransformers.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:41:51.218222Z","iopub.execute_input":"2025-04-29T07:41:51.218929Z","iopub.status.idle":"2025-04-29T07:41:51.223275Z","shell.execute_reply.started":"2025-04-29T07:41:51.218899Z","shell.execute_reply":"2025-04-29T07:41:51.222560Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'4.51.1'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"!pip install --no-cache-dir   bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:41:51.223952Z","iopub.execute_input":"2025-04-29T07:41:51.224152Z","iopub.status.idle":"2025-04-29T07:41:54.234955Z","shell.execute_reply.started":"2025-04-29T07:41:51.224138Z","shell.execute_reply":"2025-04-29T07:41:54.234230Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\nbnb_config=BitsAndBytesConfig(\n    load_in_4bit=True,\n     bnb_4bit_use_double_quant=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:41:54.236049Z","iopub.execute_input":"2025-04-29T07:41:54.236361Z","iopub.status.idle":"2025-04-29T07:41:54.250435Z","shell.execute_reply.started":"2025-04-29T07:41:54.236326Z","shell.execute_reply":"2025-04-29T07:41:54.249757Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import AutoTokenizer,AutoModelForSequenceClassification\nmodel = AutoModelForSequenceClassification.from_pretrained(\"mistralai/Mistral-7B-v0.1\",token=\"\",quantization_config=bnb_config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:41:54.251149Z","iopub.execute_input":"2025-04-29T07:41:54.251396Z","iopub.status.idle":"2025-04-29T07:43:18.928566Z","shell.execute_reply.started":"2025-04-29T07:41:54.251372Z","shell.execute_reply":"2025-04-29T07:43:18.928037Z"}},"outputs":[{"name":"stderr","text":"2025-04-29 07:41:57.346685: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745912517.369097     106 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745912517.375862     106 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b1f3005236e4139aa94b6f51b9ec28d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fb1c17703484b81b2414fa2851ae067"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e6a656a63404008831255be43d7ec3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fff887c5cc9f4e6f87403309598697dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76abdebe35544c3aaecf0f3eb83f92bd"}},"metadata":{}},{"name":"stderr","text":"Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:43:18.929276Z","iopub.execute_input":"2025-04-29T07:43:18.929727Z","iopub.status.idle":"2025-04-29T07:43:18.935177Z","shell.execute_reply.started":"2025-04-29T07:43:18.929708Z","shell.execute_reply":"2025-04-29T07:43:18.934441Z"}},"outputs":[{"name":"stdout","text":"MistralForSequenceClassification(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): MistralRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): MistralRotaryEmbedding()\n  )\n  (score): Linear(in_features=4096, out_features=2, bias=False)\n)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"for param in model.model.parameters():\n    param.require_grad=False\nfor param in model.score.parameters():\n    param.require_grad=True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:43:18.935995Z","iopub.execute_input":"2025-04-29T07:43:18.936202Z","iopub.status.idle":"2025-04-29T07:43:18.951294Z","shell.execute_reply.started":"2025-04-29T07:43:18.936185Z","shell.execute_reply":"2025-04-29T07:43:18.950666Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from peft import LoraConfig,get_peft_model\nlora=LoraConfig(\n    r=4,\n    lora_alpha=4,\n    lora_dropout=0.2\n)\nmodel=get_peft_model(model,lora)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:43:18.954102Z","iopub.execute_input":"2025-04-29T07:43:18.954311Z","iopub.status.idle":"2025-04-29T07:43:19.445546Z","shell.execute_reply.started":"2025-04-29T07:43:18.954295Z","shell.execute_reply":"2025-04-29T07:43:19.445003Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=5e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:43:19.446309Z","iopub.execute_input":"2025-04-29T07:43:19.446527Z","iopub.status.idle":"2025-04-29T07:43:19.483837Z","shell.execute_reply.started":"2025-04-29T07:43:19.446511Z","shell.execute_reply":"2025-04-29T07:43:19.483350Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(\n    train,\n    test_size=0.2,\n    random_state=42\n)\ntest, valid = train_test_split(\n    test,\n    test_size=0.5,\n    random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:43:19.484483Z","iopub.execute_input":"2025-04-29T07:43:19.484653Z","iopub.status.idle":"2025-04-29T07:43:19.515537Z","shell.execute_reply.started":"2025-04-29T07:43:19.484640Z","shell.execute_reply":"2025-04-29T07:43:19.515027Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def tokenize_texts(texts):\n    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\",padding_side='right')\n\n# Create PyTorch Dataset\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.encodings = tokenize_texts(texts)\n        self.labels = torch.tensor(labels)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item[\"labels\"] = self.labels[idx]\n        return item","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:43:19.516197Z","iopub.execute_input":"2025-04-29T07:43:19.516420Z","iopub.status.idle":"2025-04-29T07:43:19.521605Z","shell.execute_reply.started":"2025-04-29T07:43:19.516407Z","shell.execute_reply":"2025-04-29T07:43:19.520845Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\ntrain_dataset = TextDataset(train['Text'].tolist(), train['Class'].tolist())\nval_dataset = TextDataset(test['Text'].tolist(),test['Class'].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:43:19.522356Z","iopub.execute_input":"2025-04-29T07:43:19.522545Z","iopub.status.idle":"2025-04-29T07:43:24.744827Z","shell.execute_reply.started":"2025-04-29T07:43:19.522531Z","shell.execute_reply":"2025-04-29T07:43:24.744258Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:43:24.745587Z","iopub.execute_input":"2025-04-29T07:43:24.745883Z","iopub.status.idle":"2025-04-29T07:43:24.750392Z","shell.execute_reply.started":"2025-04-29T07:43:24.745841Z","shell.execute_reply":"2025-04-29T07:43:24.749636Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"num_training_steps = len(train_loader) * 3  # 3 epochs\nlr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n\n# Training loop\nEPOCHS = 1\n\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n\n    for batch in train_loader:\n        batch = {key: val.to(device) for key, val in batch.items()}\n\n        optimizer.zero_grad()\n        batch.pop('token_type_ids',None)\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n\n        total_loss += loss.item()\n\n    avg_train_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T07:43:24.751200Z","iopub.execute_input":"2025-04-29T07:43:24.751422Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"model.eval()\ncorrect = 0\ntotal = 0\npred_label=[]\nwith torch.no_grad():\n    for batch in val_loader:\n        batch = {key: val.to(device) for key, val in batch.items()}\n        batch.pop('token_type_ids',None)\n        outputs = model(**batch)\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        pred_label.append(predictions)\n        correct += (predictions == batch[\"labels\"]).sum().item()\n        total += batch[\"labels\"].size(0)\n\naccuracy = correct / total\nprint(f\"Validation Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(pred_label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"./trbt-tokenizer_mdel\")\ntokenizer.save_pretrained(\"./trbt-tokenizer_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}