{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive-லிருந்து .zip file-ஐ எடுத்துட்டு,\n",
        "# அதை /content/extracted_zip folder-க்கு unzip பண்ணும் code இது."
      ],
      "metadata": {
        "id": "EJOKWvfL-78c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zLEFCT0RBno",
        "outputId": "a23dbb07-2f5e-43ed-a1f0-8591381fe9f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipped from Drive.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/Tokenizer_cholladi_folder/cholloadai-2021.txt.zip'\n",
        "extract_path = '/content/extracted_zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Unzipped from Drive.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# /content/extracted_zip folder-ல இருக்கும் .gz files-ஐ எல்லாம்  combined_output.txt என்ற single file-ஆ convert பண்ணும் Python code இது.\n"
      ],
      "metadata": {
        "id": "dTgDXz4T_jwr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1DU0m72RBk5",
        "outputId": "6d2f562c-f8fc-4ec5-faf4-4cfc45500aaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: cholloadai-2021.txt.00.gz\n",
            "Processing: cholloadai-2021.txt.01.gz\n",
            "Processing: cholloadai-2021.txt.02.gz\n",
            "Processing: cholloadai-2021.txt.03.gz\n",
            "Processing: cholloadai-2021.txt.04.gz\n",
            "Processing: cholloadai-2021.txt.05.gz\n",
            "Processing: cholloadai-2021.txt.06.gz\n",
            "Processing: cholloadai-2021.txt.07.gz\n",
            "Processing: cholloadai-2021.txt.08.gz\n",
            "Processing: cholloadai-2021.txt.09.gz\n",
            "Processing: cholloadai-2021.txt.10.gz\n",
            "Processing: cholloadai-2021.txt.11.gz\n",
            "Processing: cholloadai-2021.txt.12.gz\n",
            "Processing: cholloadai-2021.txt.13.gz\n",
            "Processing: cholloadai-2021.txt.14.gz\n",
            "Processing: cholloadai-2021.txt.15.gz\n",
            "Processing: cholloadai-2021.txt.16.gz\n",
            "Processing: cholloadai-2021.txt.17.gz\n",
            "Processing: cholloadai-2021.txt.18.gz\n",
            "Processing: cholloadai-2021.txt.19.gz\n",
            "Processing: cholloadai-2021.txt.20.gz\n",
            "Processing: cholloadai-2021.txt.21.gz\n",
            "Processing: cholloadai-2021.txt.22.gz\n",
            "Processing: cholloadai-2021.txt.23.gz\n",
            "Processing: cholloadai-2021.txt.24.gz\n",
            "Processing: cholloadai-2021.txt.25.gz\n",
            "Processing: cholloadai-2021.txt.26.gz\n",
            "Processing: cholloadai-2021.txt.27.gz\n",
            "Processing: cholloadai-2021.txt.28.gz\n",
            "Processing: cholloadai-2021.txt.29.gz\n",
            "Processing: cholloadai-2021.txt.30.gz\n",
            "Processing: cholloadai-2021.txt.31.gz\n",
            "Processing: cholloadai-2021.txt.32.gz\n",
            "Processing: cholloadai-2021.txt.33.gz\n",
            "Processing: cholloadai-2021.txt.34.gz\n",
            "Processing: cholloadai-2021.txt.35.gz\n",
            "Processing: cholloadai-2021.txt.36.gz\n",
            "Processing: cholloadai-2021.txt.37.gz\n",
            "Processing: cholloadai-2021.txt.38.gz\n",
            "Processing: cholloadai-2021.txt.39.gz\n",
            "Processing: cholloadai-2021.txt.40.gz\n",
            "Processing: cholloadai-2021.txt.41.gz\n",
            "Processing: cholloadai-2021.txt.42.gz\n",
            "Processing: cholloadai-2021.txt.43.gz\n",
            "Processing: cholloadai-2021.txt.44.gz\n",
            "Processing: cholloadai-2021.txt.45.gz\n",
            "Processing: cholloadai-2021.txt.46.gz\n",
            "Processing: cholloadai-2021.txt.47.gz\n",
            "Processing: cholloadai-2021.txt.48.gz\n",
            "Processing: cholloadai-2021.txt.49.gz\n",
            "Processing: cholloadai-2021.txt.50.gz\n",
            "Processing: cholloadai-2021.txt.51.gz\n",
            "Processing: cholloadai-2021.txt.52.gz\n",
            "Processing: cholloadai-2021.txt.53.gz\n",
            "Processing: cholloadai-2021.txt.54.gz\n",
            "Processing: cholloadai-2021.txt.55.gz\n",
            "Processing: cholloadai-2021.txt.56.gz\n",
            "Processing: cholloadai-2021.txt.57.gz\n",
            "Processing: cholloadai-2021.txt.58.gz\n",
            "Processing: cholloadai-2021.txt.59.gz\n",
            "Processing: cholloadai-2021.txt.60.gz\n",
            "Processing: cholloadai-2021.txt.61.gz\n",
            "Processing: cholloadai-2021.txt.62.gz\n",
            "Processing: cholloadai-2021.txt.63.gz\n",
            "Processing: cholloadai-2021.txt.64.gz\n",
            "Processing: cholloadai-2021.txt.65.gz\n",
            "Processing: cholloadai-2021.txt.66.gz\n",
            "Processing: cholloadai-2021.txt.67.gz\n",
            "Processing: cholloadai-2021.txt.68.gz\n",
            "Processing: cholloadai-2021.txt.69.gz\n",
            "Processing: cholloadai-2021.txt.70.gz\n",
            "Processing: cholloadai-2021.txt.71.gz\n",
            "Processing: cholloadai-2021.txt.72.gz\n",
            " All .gz files have been concatenated into one file.\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import os\n",
        "\n",
        "folder_path = \"/content/extracted_zip\"  # change if different\n",
        "output_file_path = \"/content/combined_output.txt\"  # final combined file\n",
        "\n",
        "# Open the output file in write mode\n",
        "with open(output_file_path, 'wb') as outfile:\n",
        "    # Iterate over all .gz files in the folder\n",
        "    for filename in sorted(os.listdir(folder_path)):\n",
        "        if filename.endswith(\".gz\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            print(f\"Processing: {filename}\")\n",
        "            # Open each .gz file and write its content to the output file\n",
        "            with gzip.open(file_path, 'rb') as infile:\n",
        "                outfile.write(infile.read())\n",
        "\n",
        "print(\" All .gz files have been concatenated into one file.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **முயற்சி 1:**"
      ],
      "metadata": {
        "id": "wGRk1x7g_4qS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# combined_output.txt file-ஐ base-ஆ கொண்டு,\n",
        "# Byte-level BPE tokenizer-ஐ train பண்ணி,\n",
        "#.tokenizer_gpt folder-க்கு vocabulary (vocab.json) மற்றும் merge rules (merges.txt) files-ஆ save பண்ணும் process."
      ],
      "metadata": {
        "id": "yZP7EXE0AQ7l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWO7IMDrRBih"
      },
      "outputs": [],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "tokenizer.train(files=[\"/content/combined_output.txt\"], vocab_size=30000, min_frequency=2)\n",
        "os.makedirs('.tokenizer_gpt',exist_ok=True)\n",
        "tokenizer.save_model(\".tokenizer_gpt\")\n",
        "tokenizer.save(\".tokenizer_gpt/vocab.json\", pretty=True)\n",
        "tokenizer.save(\".tokenizer_gpt/merges.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# .tokenizer_gpt/vocab.json file-இருந்து tokenizer-ஐ load பண்ணி,\n",
        "#[PAD]ன்னு ஒரு special padding token-ஐ add பண்ணும் Python code இது."
      ],
      "metadata": {
        "id": "Qja8xdWLAofy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2cRyy5wRBd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "796cd1ee-4677-4fd3-8414-9f8114bf0443"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer_file_path = os.path.abspath(\".tokenizer_gpt/vocab.json\")\n",
        "tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_file=tokenizer_file_path\n",
        ")\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMklEwVURBaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bca416a-0be0-42bb-8c52-f23626dbb734"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['à®ĩà®ª',\n",
              " 'à¯į',\n",
              " 'à®Ł',\n",
              " 'à®¿',\n",
              " 'à®¯',\n",
              " 'à¯ĩ',\n",
              " 'Ġà®ª',\n",
              " 'à¯ĩ',\n",
              " 'à®ļ',\n",
              " 'à®¿',\n",
              " 'à®ķ',\n",
              " 'à¯į',\n",
              " 'à®ķ',\n",
              " 'à®¿',\n",
              " 'à®Ł',\n",
              " 'à¯į',\n",
              " 'à®Ł',\n",
              " 'à¯ĩ',\n",
              " 'Ġà®ĩà®°',\n",
              " 'à¯ģ',\n",
              " 'à®¨',\n",
              " 'à¯į',\n",
              " 'à®¤',\n",
              " 'à®¾',\n",
              " 'Ġà®İà®ª',\n",
              " 'à¯į',\n",
              " 'à®Ł',\n",
              " 'à®¿',\n",
              " 'Ġà®¯',\n",
              " 'à®¾',\n",
              " 'à®°',\n",
              " 'à¯ģ',\n",
              " 'Ġà®ª',\n",
              " 'à¯Ĩ',\n",
              " 'à®°',\n",
              " 'à¯ģ',\n",
              " 'à®ļ',\n",
              " 'à¯ģ',\n",
              " 'à®©',\n",
              " 'à¯ģ',\n",
              " 'Ġà®ħà®Ł',\n",
              " 'à®¿',\n",
              " 'à®ļ',\n",
              " 'à®¿',\n",
              " 'à®ķ',\n",
              " 'à¯į',\n",
              " 'à®ķ',\n",
              " 'à®¾',\n",
              " 'à®Ł',\n",
              " 'à¯į',\n",
              " 'à®Ł',\n",
              " 'à¯ģ',\n",
              " 'Ġ']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "tokenizer.tokenize('இப்டியே பேசிக்கிட்டே இருந்தா எப்டி யாரு பெருசுனு அடிசிக்காட்டு ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVrc-_yHRBXQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "87026e5d-7f43-4f6c-e8f7-5d63aa0923ea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_95a88269-2445-40f8-9df0-935896f7f5e1\", \"vocab.json\", 1407050)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('.tokenizer_gpt/vocab.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **முயற்சி 2:**"
      ],
      "metadata": {
        "id": "8QjAjZhiA-zS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BPE tokenizer creation,\n",
        "#Tamil text (combined_output.txt)ல் train பண்ணி,\n",
        "#Normalization, pre-tokenization, special tokens எல்லாம் configure பண்ணி,\n",
        "#Final output-ஆ .json format-ல் saved."
      ],
      "metadata": {
        "id": "eEIvUuZ4CQdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
        "\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "tokenizer.normalizer = normalizers.Sequence([\n",
        "    normalizers.NFKC(),\n",
        "])\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "trainer = trainers.BpeTrainer(\n",
        "\n",
        "    vocab_size=30000,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        ")\n",
        "\n",
        "tokenizer.train([\"/content/combined_output.txt\"], trainer)\n",
        "tokenizer.save(\"tamil_tokenizer.json\")\n"
      ],
      "metadata": {
        "id": "VzaU4i3s3HXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "# Print first 20 vocab tokens\n",
        "for i, (token, idx) in enumerate(vocab.items()):\n",
        "    print(f\"{idx}: {token}\")\n",
        "    if i > 20:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "OlitydzR4oEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SentencePieace-Tokenizer**"
      ],
      "metadata": {
        "id": "wohoROqM-4ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece\n"
      ],
      "metadata": {
        "id": "FDb_S3Nb-2fO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "693f53bb-4a5a-4ad9-f824-4dfaabca455e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input='/content/combined_output.txt',\n",
        "    model_prefix='tamil_unigram',\n",
        "    vocab_size=15000,\n",
        "    model_type='unigram',\n",
        "    character_coverage=1.0,\n",
        "    pad_id=0, unk_id=1, bos_id=2, eos_id=3,\n",
        "    user_defined_symbols=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "pX2vvtUV-4Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"tamil_unigram.model\")\n",
        "\n",
        "# Tokenize a sentence\n",
        "tokens = sp.encode(\"பெண்ணை பற்றி அவமதித்தார்கள்\", out_type=str)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Decode back to sentence\n",
        "decoded = sp.decode(tokens)\n",
        "print(\"Decoded:\", decoded)\n"
      ],
      "metadata": {
        "id": "t3kWTDWP-4HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zww3ryT-4FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z5hrmB08-4Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4KZACOLk-3_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5fGb2UOV-37b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kn8U2OVY-35H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YqDxAESR-319"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1-auTPOG-3zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "983aIYIu-3wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VrAlM5mI-3uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RcqCAQDO-3s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l9_Zm_z6-3pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hxU14IgT-3nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A3nltPCp-3ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cSR5nk66-3iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SYxzDEX-3fU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}